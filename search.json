[{"title":"Hexo 블로그 초간단 Markdown 이미지 업로드 방법","url":"%2F2018%2F07%2F01%2FHexo%20%EB%B8%94%EB%A1%9C%EA%B7%B8%20%EC%B4%88%EA%B0%84%EB%8B%A8%20Markdown%20%EC%9D%B4%EB%AF%B8%EC%A7%80%20%EC%97%85%EB%A1%9C%EB%93%9C%20%EB%B0%A9%EB%B2%95%2F","content":"\n### * Hexo 블로그 초간단 Markdown 이미지 업로드 방법\n---\n원래 사진 파일 업로드는 [imgur](http://imgur.com)를 이용하고 있었는데, Github 내에서 간단하게 올리고 마크다운 문법도 자동으로 만들어주는 좋은 방법이 있어서 공유한다. 무려 이미지를 무료로 업로드 하고 쓸 수 있다!\n\n![fireshot pro screen capture 006 - twosb sangbeom lee - github_com_twosb](https://user-images.githubusercontent.com/37604501/42131846-b095631c-7d46-11e8-81e9-70a6cfa8b268.png)\n\n먼저 [Github](http://github.com)에 로그인하고 `Your Profile` 을 클릭한다.\n\n![fireshot pro screen capture 002 - twosb sangbeom lee - github_com_twosb](https://user-images.githubusercontent.com/37604501/42131847-b4209ae2-7d46-11e8-836b-dae2833288b6.png)\n\n다음으로 나오는 페이지에서 `[yourname].github.io` Repository를 클릭한다.\n(필자는 twosb.github.io : 사실 어느 레포든 상관 없다.)\n\n![fireshot pro screen capture 003 - twosb_twosb_github_io - github_com_twosb_twosb_github_io](https://user-images.githubusercontent.com/37604501/42131880-2a429f68-7d47-11e8-8a6d-6bb6b976204f.png)\n\n레포에 들어가서 `Issues` 탭을 클릭한다.\n\n![fireshot pro screen capture 004 - issues twosb_twosb_github_io - github_com_twosb_twosb_github_io_issues](https://user-images.githubusercontent.com/37604501/42131887-559da78e-7d47-11e8-81b6-090895839443.png)\n\n`New issue` 버튼을 클릭한다.\n\n![fireshot pro screen capture 005 - new issue twosb_twosb_github_io - github_com_twosb_twosb_github_io_issues_new](https://user-images.githubusercontent.com/37604501/42131850-b61a5ef0-7d46-11e8-9f59-079bee6dcef5.png)\n\n위와 같은 창이 나오는데,\n\n![fireshot pro screen capture 006 - new issue twosb_twosb_github_io - github_com_twosb_twosb_github_io_issues_new](https://user-images.githubusercontent.com/37604501/42131851-b6423f92-7d46-11e8-9d59-74019a3d2b13.png)\n\n사진 파일을 `Leave a comment` 가 적혀있는 칸에 끌어다 놓는다.(복수 파일 가능)\n\n![fireshot pro screen capture 007 - new issue twosb_twosb_github_io - github_com_twosb_twosb_github_io_issues_new](https://user-images.githubusercontent.com/37604501/42131852-b8cb6874-7d46-11e8-86e6-fc11ed94ee62.png)\n\nUploading 어쩌고가 나오면 조금만 기다려주자..\n\n![fireshot pro screen capture 008 - new issue twosb_twosb_github_io - github_com_twosb_twosb_github_io_issues_new](https://user-images.githubusercontent.com/37604501/42131853-b8f1b4b6-7d46-11e8-8402-2292ee731b3e.png)\n\n완료! `Markdown` 문법대로 이미지 파일 경로를 만들어주었다. 개꿀!\n\n![default](https://user-images.githubusercontent.com/37604501/42131904-c50d975a-7d47-11e8-97ac-3af89f517c4c.png)\n\n이런식으로 작성을 끝내면 `Submit new issue` 를 하지 않고 그대로 뒤로 가기를 눌러도 업로드 된 파일은 남아있다.\n\n앞으로 `Typora` 대신 `Marp + Github image upload` 전략을 사용해야겠다. 개꿀이다 이건!","tags":["이미지 무료 업로드"],"categories":["꿀팁"]},{"title":"Selenium을 이용한 네이버 기사 10페이지 제목 리스트 스크레이핑!(크롤링)","url":"%2F2018%2F06%2F27%2FSelenium%20%EB%84%A4%EC%9D%B4%EB%B2%84%20%EA%B8%B0%EC%82%AC%20%EC%8A%A4%ED%81%AC%EB%A0%88%EC%9D%B4%ED%95%91%2F","content":"\n#### *** Selenium을 이용한 네이버 기사 10페이지 제목 리스트 스크레이핑!(크롤링)**\n\n***\n\n실제 개발 시 구글링을 통해 참고하다보니 쓸데 없이 긴 글 보다 짧지만 정확한 정보가 있는 글이 몰입감이 훨씬 높다는 사실을 깨달았다. 그래서 필요한 과정만 정확히 쓰는 습관을 들이도록 하겠당.. = =\n\n저번 포스팅에서 예고했던 것 처럼, 스크레이핑 툴인 [셀레니움](https://www.seleniumhq.org)을 이용해 [네이버 IT/과학 홈](http://news.naver.com/main/main.nhn?mode=LSD&mid=shm&sid1=105>)에 있는 기사 제목 200개를 스크레이핑 해보겠다. IDE은 주피터 노트북을 사용하겠다.\n\n> 주피터 노트북은 터미널 창에서 `pip install jupyter` 를 입력하면 설치 할 수 있고, 파일들이 있는 폴더에서 [Bash shell](https://git-scm.com/downloads)을 실행해 `jupyter notebook` 을 입력하면 실행시킬 수 있다.\n\n\n\n![](https://i.imgur.com/EcfF4uQ.png)\n\n<u>*** 바로 이 부분의 제목을 스크레이핑 할 것이다.**</u>\n\n\n\n##### 1. 스크레이핑을 위한 Selenium 설치\n\n[이곳](https://zetawiki.com/wiki/%EC%9C%88%EB%8F%84%EC%9A%B0_%EC%85%80%EB%A0%88%EB%8A%84_%EC%84%A4%EC%B9%98)에 셀레니움 설치 방법이 나와있다. 조금 수정하자면\n\n- 현재 ChromeDriver는 최신 버전이 2.4다. 받아준다.\n- chromedriver.exe를 굳이 D:\\로 이동해주지 않아도 된다. 아래 예제에 경로가 이렇게 나와 있는데, D:/chromedriver에 해당하는 부분을 chromedriver.exe 파일이 있는 경로로 수정만 하면 된다.\n\n```\ndriver = webdriver.Chrome('D:/chromedriver') # D:/chromedriver 부분을 수정해주자.\n```\n\n<br>\n\n설치가 끝났으면 주피터 노트북을 켜고 New file을 만들어주고, 첫 줄에\n\n```\nfrom selenium import webdriver\nimport time\ndriver = webdriver.Chrome()\n```\n\n를 입력해주자. 동작을 하지 않는다면 () 안에 들어갈 부분을 적절하게 수정해주자. 그럼 아래와 같은 창이 뜬다.\n\n![](https://i.imgur.com/ZatGZAk.png)\n\n`Chrome이 자동화된 테스트 소프트웨어에 의해 제어되고 있습니다.` 라는 멘트가 의미심장하다. 바로 이 새로운 Chrome 창에서 우리가 입력하는 명령어들이 수행된다.\n\n`import` 는 추후 페이지를 이동 할 때 1초의 Delay를 걸어주기 위한 모듈이다. 일단 임포트 해주자.\n\n<br>\n\n##### 2. 스크레이핑 할 네이버 기사 타이틀을 긁어오기\n\n```\ndriver.get(\"http://news.naver.com/main/main.nhn?mode=LSD&mid=shm&sid1=105\")\n```\n\n`get` 명령어를 통해 해당 URL로 이동한다. 그럼 자동화된 Chrome창이 아래와 같이 보일 것이다.\n\n![](https://i.imgur.com/BHlco8C.png)\n\n이어서\n\n``` \ntime.sleep(1)\n```\n\n코드를 추가해준다. 웹 페이지가 읽어들여질 때 까지 잠깐 기다려 주는 것이다. `get` 으로 웹페이지를 읽어준 후 추가해주는게 가장 효율적이다.\n\n```\narticles = driver.find_elements_by_css_selector('#section_body > ul > li > dl > dt > a')\n```\n\n그리고 기사 목록을 읽어들여준다.  `find_elements_by_css_selector` 명령어를 사용해주는데, [저번 포스팅](https://bit.ly/2ySyRcS)에서 사용했던 `requests` 모듈을 이용한 방식이 아닌 `CSS Selector` 방식을 사용하겠다.\n\n> CSS Selector에 대해 잘 모르겠다면, [이 곳](http://www.nextree.co.kr/p8468/)을 참고하자. 너무나도 잘 설명되어 있다. \n\n<br>\n\n![](https://i.imgur.com/zBdCpK8.png)\n\n`F12` 를 눌러 개발자 도구로 들어간다. 그리고 `빨간색 원` 부분을 눌러준다.  단축키로는 `Ctrl+Shift+C` 이다. \n\n![](https://i.imgur.com/35EdF2c.png)\n\n스크롤을 아래로 내리면 기사가 시작되는 부분 가장 위에 `배틀그라운드 모바일, 매출 순위 다시 급상승` 이라는 기사 제목이 있다.(물론 기사가 계속 바뀌므로 다른 제목이 보일 것이다.)  이를 눌러주면 오른쪽 연한 파란 상자가 해당 제목이 들어가있는 html 코드 부분을 선택해준다.\n\n![](https://i.imgur.com/P2orvPJ.png)\n\n오른쪽 연한 파란 상자 부분을 오른쪽으로 클릭한 후 `Copy -> Copy selector ` 을 클릭하면\n\n**`#section_body > ul.type06_headline > li:nth-child(1) > dl > dt:nth-child(2) > a`** \n\n가 복사된다. 오른쪽으로 갈 수록 하위 코드로 내려가게 된다. 하나하나 간단하게 뜯어보자.\n\n<br>\n\n1. #section_body : 앞에 `#` 는 `id` 를 나타낸다. \n\n![](https://i.imgur.com/QOU0PXg.png)\n\n왼쪽은 `id=\"section_body\"` 가 해당되는 범위이고, 오른쪽 파란 박스는 해당 id가 포함된 코드 부분이다.\n\n`class=\"section_body\"` 를 나타내는 `.section_body` 로 대체 할 수 있다.\n\n<br>\n\n2. ul.type06_headline : `ul` 뒤의 `.` 은 `type06_headline` 의 `class` 를 나타낸다.\n\n![](https://i.imgur.com/d4KU1HG.png)\n\nHTML에서 목록을 나타내는 태그인 `ul` 는 순서가 **없는** 목록이며,  `ol` 은 순서가 **있는** 목록이다.\n\nul의 u는 위로 구멍이 뚫려 있으므로 제한이 없고, ol의 o는 구멍이 뚫려있지 않으므로 제한이 없다고 기억하자.\n\n`ul.type06_headline` = `ul` + `.` + `type06_headline` 이라고 생각하면 된다.\n\n<br>\n\n3. li:nth-child(1) : `li` 태그 밑에 붙는 `nth-child(1)` 에 대해 궁금하다면 [이 곳](https://bit.ly/2It4Yzr)을 참고한다. 카인드데스네~\n4. dl\n5. dt:nth-child(2)\n6. a\n\n들의 설명은 생략하겠다.. 링크 글을 읽었다면 쉽게 이해할 수 있다. 모르면 댓글로..\n\n<br>\n\n결국, `#section_body > ul.type06_headline > li:nth-child(1) > dl > dt:nth-child(2) > a`  CSS Selector 코드는 `'#section_body > ul > li > dl > dt > a'` 코드로 변경된다. (`#section_body` 는 `.section_body` 로 변경해도 상관 없다.)\n\n<br>\n\n#### 3. 10페이지 기사 제목을 모두 담기\n\n이를 `article_Scraping` 함수로 만들면 다음과 같다.\n\n```\narticle_list = []\ndef article_Scraping():\n    driver = webdriver.Chrome()\n    for i in range(1, 11):\n        driver.get(\"http://news.naver.com/main/main.nhn?\\\n        mode=LSD&mid=shm&sid1=105#&date=%2000:00:00&page=\" + str(i))\n        time.sleep(1)\n        articles = driver.find_elements_by_css_selector('#section_body > ul > li > dl > dt > a')\n        for article in articles:\n            if article.text == \"동영상기사\":\n                pass\n            elif len(article.text) != 0:\n                article_list.append(article.text)                \n    driver.quit()\n    for article in article_list:\n        print(article)\n                \narticle_Scraping()\n```\n\n1. `article_list` : 1부터 10페이지의 모든 제목이 담길 list이다.\n2. `article_Scraping` : 10페이지 기사 제목을 담을 명령을 가진 함수를 만들었다.\n3. `driver` : `chromedriver` 실행!\n4. `for문 (1)` : 1부터 10페이지까지 1페이지 당 20개의 기사 제목들을 `article_list` 에 담는 노가다를 해준다.\n5. `driver.get` : 페이지를 이동해준다. 1페이지 주소는 아래와 같이 이루어져 있다.\n\n![](https://i.imgur.com/ZXhfxXt.png)\n\n끝에 있는 `page=1 `에  `1` 부분만 `1, 2, 3, 4, 5, 6, 7, 8, 9, 10` 으로 바꿔주면 된다.\n\n그래서 `for문 (1)` 의 `range` 를 1부터 11까지 잡은 것이다.\n\n6. `time.sleep` : get 명령어로 1초 동안 다음 명령어로 움직이지 않고 쿨쿨 자준다.\n7. `articles` : `CSS Selector `로  기사가 있는 경로로 접근해준다. 최종 도착점은 `a` 태그이다. 하지만, `articles `를 무턱대고 출력하면 아래와 같은 결과가 나온다.\n\n![](https://i.imgur.com/gaMYQMZ.png)\n\n모든 텍스트를 출력하다보니 아무 내용 없는 `\"\"` 부분도 출력해버리게 된다. 걸러야 할 부분은 두 가지이다.\n\n- `\"동영상기사\"` 라는 내용이 입력된 기사의 Text\n- `\"\"` 로서 Text 길이가 0인 Text\n\n8. `for article in articles:` : 첫 번째로 `article.text` 가 `\"동영상기사\"` 인지 판단하여 맞으면 `pass`.\n   두 번째로 `article.text` 의 길이가 `0` 이 아니면(0이면 pass인거임) `article_list` 에 `append`.\n   두 조건의 순서가 뒤바뀌면 안된다. \n9. `driver.quit` : `article_list` 에 모든 기사 제목이 입력되었기 때문에 `chromedriver` 를 끈다.\n10. `article_list` 에 있는 모든 `article` 을 출력한다.\n11. `article_Scraping` 함수를 실행한다.\n\n결과는 아래와 같다.\n\n![](https://i.imgur.com/ykRNqMp.png)\n\n기사 제목이 한 줄씩 출력된다. 1페이지에 20개 기사씩, 총 200개의 기사 제목이 출력되는 것이다.\n\n<br>\n\n> 어떻게 보면 기초적인 스크레이핑지만, 생각보다 자잘한 조건들을 찾아서 넣어줘야 했다.\n>\n> 스크레이핑(크롤링)은 노가다다. 노가다를 즐기도록 하자.\n\n<br>\n\n*** 더 궁금한 부분이 있다면, 댓글을 남겨주세요. 바로 대답 드리겠습니다.**","tags":["NAVER"],"categories":["PYTHON"]},{"title":"STATS.NBA.COM 크롤링(스크래핑) by using JSON in Python","url":"%2F2018%2F06%2F24%2FSTATS.NBA.COM%20%ED%81%AC%EB%A1%A4%EB%A7%81(%EC%8A%A4%ED%81%AC%EB%9E%98%ED%95%91)%20by%20using%20JSON%20in%20Python%2F","content":"\n### **STATS.NBA.COM 크롤링(스크래핑) by using JSON in Python**\n\n***\n\n<br>\n\n> STATS.NBA.COM 파싱하기 드러워서 내가 쓴다...\n>\n> 크롤링인지 스크래핑인지 만만치 않다는 사실을 깨달았읍니다...\n\n<br>\n\n``` \nQuestion: 아래 URL의 NBA 데이터를 크롤링하여 판다스 데이터 프레임으로 나타내세요.\n```\n\n해당 페이지 링크는 아래과 같다.\n\n[http://stats.nba.com/teams/traditional/?sort=GP&dir=-1](http://stats.nba.com/teams/traditional/?sort=GP&dir=-1)\n\n<br>\n\n그리고 우리가 Pandas로 구현해야 하는 \n\n![](https://i.imgur.com/xGM5D4h.png)\n\n이 테이블을 JSON 파일 형태로 html Pasing하여 출력하도록 하겠다.\n\n\n\n#### 1. STATS.NBA.COM 해당 페이지의 숨어있는 JSON 파일 찾기 \n\n![](https://i.imgur.com/5QjmDV1.png)\n\n위에 표시된 링크에 들어가면 이러한 화면이 보인다.\n\n<br>\n\n![](https://i.imgur.com/BqssKMP.png)\n\n이곳에서 키보드의 `F12` 를 누르면, 오른쪽에 개발자 도구가 뜨게 되는데, `Network` 에 들어가서 `XHR` 탭을 클릭하고 `F5` 를 눌러보자.\n\n<br>\n\n![](https://i.imgur.com/nmejVJ3.png)\n\n`Name` 탭에 5가지의 Request 결과가 뜨는데, 이 중 4번째인 `leaguedashteamstats?Conference=` 로 시작하는 데이터를 클릭하고 오른쪽 탭의 `resultSets` 를 계속 펼쳐보면 `rowSet` 묶음에 STATS.NBA.COM 페이지에서 봤던 익숙한 NBA 팀 이름들이 뜬다.\n\n개발자 도구 안에서도 JSON의 구조를 파악하고 Pasing이 가능하지만, 우리는 JSON 파일을 예쁘게 볼 수 있는 크롬 앱을 사용해보도록 하겠다. 필자가 쓰는 JSON Viewer는 [Json Viewer](https://github.com/tulios/json-viewer)다. [이 곳](https://chrome.google.com/webstore/detail/json-viewer/gbmdgpbipfallnflgajpaliibnhdgobh)에서 크롬 앱에 추가할 수 있다. 일단 추가하면 JSON 파일을 열 때 알아서 예쁘게 보여준다.\n\n그럼 이제 leaguedashteamstats?어쩌구를 더블클릭해서 열어보자... [혹은 이 링크](http://stats.nba.com/stats/leaguedashteamstats?Conference=&DateFrom=&DateTo=&Division=&GameScope=&GameSegment=&LastNGames=0&LeagueID=00&Location=&MeasureType=Base&Month=0&OpponentTeamID=0&Outcome=&PORound=0&PaceAdjust=N&PerMode=PerGame&Period=0&PlayerExperience=&PlayerPosition=&PlusMinus=N&Rank=N&Season=2017-18&SeasonSegment=&SeasonType=Playoffs&ShotClockRange=&StarterBench=&TeamID=0&VsConference=&VsDivision=)를 누르면 JSON 형태의 웹 페이지가 보인다.\n\n<br>\n\n![](https://i.imgur.com/pvYEyNv.png)\n\n이것이 바로 우리가 데이터를 뽑아내야 할 JSON 웹 페이지의 구조이다. 오른쪽 위에 보이는 위아래 화살표 버튼을 클릭하면\n\n<br>\n\n![](https://i.imgur.com/ZV60C31.png)\n\n이런 식으로 접히게 된다. 정신산란할 땐 접어놓고 복습호흡을 내뱉으며 하나하나 뜯어가며 코드를 분해하도록 하자.\n\n이정도 정보를 얻었으면, 코드를 조금 작성해보자. IDE는 Jupyter Notebook을 사용하였다.\n\n<br>\n\n```\nimport requests\nimport json\nimport pandas as pd\n```\n\nimport할 패키지는 총 3가지이다.\n\n- 원하는 웹 페이지에 Request(요청)를 보내 html 결과를 받기 위한 `requests`\n- html 안의 text 결과를 받아와서 데이터로 사용하기 위한 `json`\n- 그리고 DataFrame으로 데이터를 테이블 형태로 만들어줄 `pandas`.\n  - `as pd` 를 붙이는 이유는 pandas를 매번 입력하기 귀찮아서 pd로 퉁쳐주기 위한 것이다.\n\n<br>\n\n```\nurl = \"http://stats.nba.com/stats/leaguedashteamstats?Conference=&DateFrom=&DateTo=&Division=&GameScope=&GameSegment=&LastNGames=0&LeagueID=00&Location=&MeasureType=Base&Month=0&OpponentTeamID=0&Outcome=&PORound=0&PaceAdjust=N&PerMode=PerGame&Period=0&PlayerExperience=&PlayerPosition=&PlusMinus=N&Rank=N&Season=2017-18&SeasonSegment=&SeasonType=Playoffs&ShotClockRange=&StarterBench=&TeamID=0&VsConference=&VsDivision=\"\n```\n\n우선 url 변수에 JSON 파일의 url 주소를 입력해준다. 그리고 아래 get_data 함수의 Parameter로 써준다.\n\n<br>\n\n```\ndef get_data(url):\n    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64)\\\n    AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36'}\n    response = requests.get(url, headers=headers)\n    json_info = json.loads(response.text)\n    stats = json_info[\"resultSets\"][0][\"rowSet\"]\n    df = pd.DataFrame(columns=[\\\n    \"TEAM\",\"GP\",\"W\",\"L\",\"WIN%\",\"MIN\",\"PTS\",\"FGM\",\"FGA\",\\\n \t\"FG%\",\"3PM\",\"3PA\",\"3P%\",\"FTM\",\"FTA\",\"FT%\",\"OREB\",\"DREB\",\\\n \t\"REB\",\"AST\",\"TOV\",\"STL\",\"BLK\",\"BLKA\",\"PF\",\"PFD\",\"+/-\"])\n    for stat in stats:\n        df.loc[len(df)] = {\n            \"TEAM\":stat[1],\n            \"GP\":stat[2],\n            \"W\":stat[3],\n            \"L\":stat[4],\n            \"WIN%\":stat[5],\n            \"MIN\":stat[6],\n            \"PTS\":stat[26],\n            \"FGM\":stat[7],\n            \"FGA\":stat[8],\n            \"FG%\":stat[9],\n            \"3PM\":stat[10],\n            \"3PA\":stat[11],\n            \"3P%\":stat[12],\n            \"FTM\":stat[13],\n            \"FTA\":stat[14],\n            \"FT%\":stat[15],\n            \"OREB\":stat[16],\n            \"DREB\":stat[17],\n            \"REB\":stat[18],\n            \"AST\":stat[19],\n            \"TOV\":stat[20],\n            \"STL\":stat[21],\n            \"BLK\":stat[22],\n            \"BLKA\":stat[23],\n            \"PF\":stat[24],\n            \"PFD\":stat[25],\n            \"+/-\":stat[27],\n        }\n        df = df.sort_values(by=[\"GP\"], ascending=False)\n        df = df.reset_index(drop=True)\n    df.index += 1\n    return df\n```\n\n 하나씩 뜯어가면서 설명하겠다. 아래 코드는 위의 코드와 중복된다. 그러니 위의 코드만 입력하면 된다. 그냥 설명용으로 작성한다.\n\n<br>\n\n```\n    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64)\\\n    AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36'}\n```\n\nSTATS.NBA.COM은 아마도 Scraping을 하기 위해 필요로 하는 웹 브라우저 버전이 따로 있는 것 같다. 필자는 Chrome 67.0.3396.87 64Bit 버전을 사용하는데, 61.0.3163.100 버전을 사용하는 것 처럼 속여줘야 데이터를 가져오는 듯 하다. 그냥 구글링 해서 나의 고통을 가지고 있는 분의 질문에 대한 해답에 이런 늬앙스의 글이 있었다. 일단 headers 변수에 해당 내용을 입력해주자.\n\n<br>\n\n```\n    response = requests.get(url, headers=headers)\n    json_info = json.loads(response.text)\n```\n\nresponse 변수에 아까 입력한 url 주소의 html을 get 방식으로 받겠다고 요청을 보낸다. json_info 변수에 response변수로 받은 html내용의 text를 load하겠다는 명령어를 입력해준다.\n\n<br>\n\n```\n    stats = json_info[\"resultSets\"][0][\"rowSet\"]\n```\n\n가장 고생을 많이 한 부분이다. stats 변수에 rowSet의 데이터가 입력되어야 하는데(NBA 16팀), `json_info[\"resultSets\"][\"rowSet\"]` 으로 썼더니 `'str' object has no attribute 'read'` 에러가 떴다. [0]을 중간에 추가해줘야한다. JSON Viewer로 보면 이 구조가 잘 안보이고, 오히려 개발자 도구에서 본 구조에서 힌트를 얻었다.\n\n<br>\n\n```\n\tdf = pd.DataFrame(columns=[\\\n    \"TEAM\",\"GP\",\"W\",\"L\",\"WIN%\",\"MIN\",\"PTS\",\"FGM\",\"FGA\",\\\n \t\"FG%\",\"3PM\",\"3PA\",\"3P%\",\"FTM\",\"FTA\",\"FT%\",\"OREB\",\"DREB\",\\\n \t\"REB\",\"AST\",\"TOV\",\"STL\",\"BLK\",\"BLKA\",\"PF\",\"PFD\",\"+/-\"])\n```\n\ndf 변수 주소값에 pandas DataFrame을 할당해준다. 상단 columns에 TEAM, GP, W 등의 변수들을 추가해준다. 그럼 아래와 같은 형태로 나타난다.\n\n![](https://i.imgur.com/xgm6P2X.png)<br>\n\n```\n    for stat in stats:\n        df.loc[len(df)] = {\n            \"TEAM\":stat[1],\n            \"GP\":stat[2],\n            \"W\":stat[3],\n            \"L\":stat[4],\n            \"WIN%\":stat[5],\n            \"MIN\":stat[6],\n            \"PTS\":stat[26],\n            \"FGM\":stat[7],\n            \"FGA\":stat[8],\n            \"FG%\":stat[9],\n            \"3PM\":stat[10],\n            \"3PA\":stat[11],\n            \"3P%\":stat[12],\n            \"FTM\":stat[13],\n            \"FTA\":stat[14],\n            \"FT%\":stat[15],\n            \"OREB\":stat[16],\n            \"DREB\":stat[17],\n            \"REB\":stat[18],\n            \"AST\":stat[19],\n            \"TOV\":stat[20],\n            \"STL\":stat[21],\n            \"BLK\":stat[22],\n            \"BLKA\":stat[23],\n            \"PF\":stat[24],\n            \"PFD\":stat[25],\n            \"+/-\":stat[27],\n        }\n```\n\nfor문으로 각 행에 column과 일치하는 데이터를 순서대로 넣어준다. 나머지는 다 순서대로인데, `PTS` 탭만 stat[26] 데이터를 차용한다. 헷갈리지 말자. 여기까지 입력했으면 아래와 같이 df 변수의 DataFrame이 만들어진다.\n\n![](https://i.imgur.com/cut2GP9.png)\n\n<br>\n\n```\n        df = df.sort_values(by=[\"GP\"], ascending=False)\n        df = df.reset_index(drop=True)\n    df.index += 1\n    return df\n```\n\n만들어진 df를 `GP` column 기준으로 정렬해주고, `ascending=False` 명령어로 내림차순을 적용시켜준다. 그럼 아래와 같이 인덱스값이 흐트러지게 된다.\n\n![](https://i.imgur.com/OLYfIdi.png)\n\n`df = df.reset_index(drop=True)` 명령으로 인덱스를 리셋해주고, 기존 인덱스는 drop해버린다. 더하여 인덱스가 0으로 시작하는데, `df.index += 1` 명령으로 인덱스에 모든 값을 1씩 더해준다. indent 위치를 주의하자. 이전 명령들의 indent보다 space 4칸이 적다. 그래야 모든 index에 적용된다. 마지막으로 df를 return해준다.\n\n결과는...\n\n<br>\n\n![img](https://i.imgur.com/SUXCJHP.png)\n\n위와 같다.\n\n![](https://i.imgur.com/xGM5D4h.png)\n\n홈페이지의 이 표와 비교하면 내용은 완벽히 같다.. 추후 STATS.NBA.COM의 JSON url 값이나 column 고유값이 변하지 않는 이상, 팀간 GP 값의 변동이 있더라도 get_data 함수가 알아서 Parsing 해줄 것이다.\n\n초기에 [http://stats.nba.com/teams/traditional/?sort=GP&dir=-1](http://stats.nba.com/teams/traditional/?sort=GP&dir=-1) URL에서 보듯이, 이 예제에서는 GP값을 기준으로 정렬했는데 `df = df.sort_values(by=[\"GP\"], ascending=False)` 명령에서 `\"GP\"` 부분을 다른 column 값으로 수정하면 다른 기준으로 정렬 될 것이다.\n\n<br>\n\n> 스크래핑, 혹은 크롤링이라고 불리는 작업이 쉽지 않다는 것을 깨달은 하루였다... T-T\n>\n> 소요 시간은 약 4시간정도. 그리고 이 글을 적는 것은 1시간 반 정도 걸렸다.ㅋㅋㅋㅋ\n>\n> 무려 5시간 넘게 뺏어간 놈이다.. 하지만 추후 크롤링을 더 수월하게 할 수 있을 것이라는 자신감이 생겼다.\n\n<br>\n\n### 다음 글 예고\n\n***\n\n- **Selenium을 이용한 네이버 기사 10페이지의 제목 리스트 크롤링!**","tags":["STATS.NBA.COM"],"categories":["PYTHON"]},{"title":"나노휠 나노퀵 NQ-AIR 1000W+ 13Ah 후기! (등판각 위주)","url":"%2F2018%2F06%2F20%2F%EB%82%98%EB%85%B8%ED%9C%A0%20%EB%82%98%EB%85%B8%ED%80%B5%20NQ-AIR%201000W%2B%2013Ah%20%ED%9B%84%EA%B8%B0!%20(%EB%93%B1%ED%8C%90%EA%B0%81%20%EC%9C%84%EC%A3%BC)%2F","content":"\n#### 나노휠 나노퀵 NQ-AIR 1000W+ 13Ah 후기! (등판각 위주)\n\n***\n\n1. 중대까지 어떻게 갈 것인가 고민고민하다가.. 스쿠터 대신 전동킥보드를 골랐다. 그 중, 싱글에 1000W 파워를 가지고 있는 나노퀵 AIR 1000W를 골랐다.\n\n![](https://i.imgur.com/WVv4L6g.png)\n\n2. 고른 이유는 신품 가격은 좀 비싼데(최저가 71만원 수준) 중고가가 굉장히 싸다! 그래서 샀는데.. 문제가..ㅠㅜ 있다가 서술하겠다. 아무튼 1000km 정도 탄 것을 45만원에 구매했다.\n\n   ![img](https://i.imgur.com/QqI422T.png)\n\n   `제조회사 - 나노휠` : 극악의 A/S로 유명하지만.. 중고로 샀고 어차피 소모품이라는 생각에 크게 신경쓰지는 않았다. 어쨌든 서울이나 광명에 A/S 센터가 있기에 문제가 생기면 가면 되겠지.\n\n   `최고속도 - 55~60 ` : 아니다! 전원 버튼과 모드 버튼을 동시에 꾸욱 눌러서 설정 모드로 들어가 P7로 맞추면 실제 GPS속도와 비슷하게 나오는데, 약 50km 정도라고 생각하면 된다. 47km까진 봤다.\n\n   `주행거리 - 70km` : 뭐지...ㅎ? 뻥스펙이 심하다. 정속이 아닌 고속 주행 기준 30km 정도는 아니지만 40km는 안될 것이라고 생각된다. 실제로 30km정도 타고 배터리 1칸~1칸 반 남았다.\n\n   `충전시간 - 6~10시간` : 충전중이다. 잘때 꽂아놓고 탈때 뽑으면 되겠지 뭐.\n\n   `등판각도` : 아주아주 중요하다! 다른 전동킥보드를 타보진 않았지만, 약 20도까진 가능하다고 생각한다. 심지어 10도까지는 30km 정도로 올라갈 수 있는걸 확인했다! 놀라웠다. 정말 힘 좋다. 타고 다니는 코스 기준으로 등판각이 가장 중요했기 때문에 만족만족.\n\n   `배터리 - 리튬이온` : 흫..... 싸구려 중국제를 쓰는걸로 알고있다. 타다보면 타사 배터리(LG, 삼성)보다 성능 저하가 굉장히 빠르다고.. 교체하려면 30만원인가? 걍 버리고 다른거 사든지 스쿠터로 가든지 차던대로 차나 타든지 해야지.\n\n   `배터리 용량 - 13A` : 실제로 아쉬운 부분이다. 최대 40km 정도는 달릴 수 있긴 한데, 50km 정도였으면 좋겠다. 아아.. 인간의 욕심은 끝이 없고..\n\n   `휠 사이즈 - 10인치` : 음 확실히 8인치 제품보다는 낫다! 스쿠터 정도의 안정감을 주진 않지만, 크게 출렁이는 도로상태를 빼고는 서울에서 타기에는 매우 훌륭하다. 단, 튜브리스 타이어라 펑크 가능성은 좀 높다..\n\n   `무게 - 23kg` : 아! 집이 4층인데 팔 빠지는 줄 알았다! 집 1층에 차가 있어서 뒷트렁크에 넣을 생각으로 샀는데, 생각보다 집에서 충전할 일이 가끔 생길 것 같아 불안하다..ㅠㅠ 진짜진짜 무겁다. 안장까지 합치면 25kg다. 진짜 무겁다. 다시 생각해보자.\n\n   `접이식` : 이라고는 하지만 원터치가 아니어서 익숙해져도 최소 1분 정도는 걸릴 것 같다. 원터치 전동킥보드들이 부러워진다..\n\n   > 그 외에 쓰로틀 위치가 너무 상단에 위치해서 계속 땡기고 있으면 손가락과 손목이 아프다! 이건 어떻게 수정해야하나...ㅜ0ㅜ 그리고 안장도 좀 낮다.\n\n   \n\n![](http://img.danawa.com/images/descFiles/4/594/3593349_1522635625393.jpeg)\n\nhttps://youtu.be/1irvQi-KZ4s\n\n\n\n**사진과 유튜브를 보면 알겠지만 등판각은 오져버린다! 저것보다 속도 더 낼 수 있다. 진짜다.**\n\n\n\n3. 근데 좀 빡치는건.. 뒷바퀴쪽에서 계속 끼긱~ 끼긱~ 소리가 난다. 아마 결합을 너무 세게 했거나 너무 약하게 한 것 같은데, 어느 부위인지 모르니.. 구글갓의 힘을 빌려서 반드시 해결해야겠다. 끌어도, 천천히 가도, 빨리 가도 계속 귓구멍을 세게 때린다. -_-\n\n\n\n4. 오늘의 코스는 다음과 같다.\n\n![](https://i.imgur.com/PB6dSmP.png)\n\n- 출발 코스 : 약 9km. 길동역 - 천호역 - 광나루역 - 건대입구역 - 성수역\n\n![](https://i.imgur.com/TGkSe1T.jpg)\n\n- 복귀 코스 : 길 살짝 잘못 들어서 약 20km정도.. 성수역 - 압구정로데오 - 한남 - 동작 - 이수역\n\n\n\n이제 이거 타고 중대 언덕 함 넘어봐야 쓰겄다! 헬멧에 달 고프로 하나 장만해야 하나..? ㅎㅎㅎ","tags":["등판각"],"categories":["제품 Review"]},{"title":"중앙대학원 컴퓨터공학과 합격!","url":"%2F2018%2F06%2F19%2F%EC%A4%91%EC%95%99%EB%8C%80%ED%95%99%EC%9B%90%20%EC%BB%B4%ED%93%A8%ED%84%B0%EA%B3%B5%ED%95%99%EA%B3%BC%20%ED%95%A9%EA%B2%A9%2F","content":"\n#### 중앙대학원 컴퓨터공학과 합격! (Feat. 나노휠 Air 1000W 전동킥보드)\n\n***\n\n1. 사실 기대한 건 아니었다! 특히나 문과 학부생+30살이라는 조건때문에 공대쪽 일반대학원에 진학할 수 있을거라는 생각을 하지 않았다. 그래서 연세 정보전문대학원도 지원을 했던거고.. 하지만 오히려 그 곳은 떨어지고 중대 컴퓨터공학과에 붙었다. 지원자수는 내가 마지막이었으니 총 22명, 아마 떨어진 사람이 없지 않을까..? 라는 생각을 한다. 어쨌든간에 일반대학원으로 진학하게 된 것은 좋은 일이다.\n\n   \n\n2. 한가지 더 좋은 이유는 중대는 집에서 가깝다는 것이다. 단연코 이게 학교 선택의 이유로 작용해서는 안되겠지만, 기왕 된거 가까운게 좋은 것 아니겠는가? 그러나 **석사생은 주차장 이용이 불가능하다.** 이 문제를 타개하기 위해 두가지 선택지를 고르게 되었다.\n\n- <u>대중교통을 이용</u>한다 : 최소 2번의 버스 환승과 마의 중대 언덕을 버스 타고 다니려니 혼절각. 시간도 더 오래 걸린다.\n\n- <u>또 다른 탈 것</u>을 산다 : 스쿠터를 우선 봤으나 어차피 아침 저녁 헬 시간에는 막힐 것 같았다. 그렇다면 다른 수단은 없을까?\n\n  그리하여 찾아낸 것이 바로 **전동킥보드**다. 특히나 남성역 - 숭실대 - 중대 언덕을 넘기 위해서는 **등판각**이 좋은 녀석을 찾아야 했기 때문에-물론, 현충원으로 다니는 방법도 있지만- 3시간 정도 서치한 결과, 마감과 A/S, 배터리 수명은 헬이지만 중고값이 낮고 타다가 걍 버려도 될 정도의 모델을 찾았다.\n\n   \n\n  ![](http://img.danawa.com/prod_img/500000/182/440/img/5440182_1.jpg?shrink=500:500&_v=20180110160317)\n\n  \n\n  [나노휠 나노퀵 NQ-AIR 1000W+ 13Ah](http://prod.danawa.com/info/?pcode=5440182&keyword=nq-air%201000w#bookmark_product_information)\n\n  알고보니 다나와 전동킥보드 판매 랭킹 1위더라.. -_- 왜 네이버  카페부터 가입해서 열심히 찾았나 몰라. 암튼 거기서는 대차게 까고 있는 모델인데 실사용 유저들이 갑툭튀해서 저는 잘 타고 있어요~ 하는 댓글을 믿기로 했다. 믿는 자에게 복이 온다니 안라하겠지 뭐.(추가 : 1000W+가 아니라 1000W이었슴다. 눈물..)\n\n  코스는 대강 아래와 같다.\n\n  ![](https://i.imgur.com/QjDmyW5.png)\n\n  이 숭실대 - 중앙대 뒤편 달마사 부근 언덕.. 무섭긴 한데 그래도 잘 탈 수 있겠지?\n\n\n\n3. 중대 대학원 합격 후 컨택을 하느라 아주 심신이 지쳤었다. 보통 공대의 테크트리는 선 컨택 - 후 합격 구조인데, 난 자대도 아닌데다가 문과 출신이라 전혀 기대도 하지 않았기 때문이다. 덕분에 [이 페이지](http://cse.cau.ac.kr/20141201/sub03/sub0302.php)만 하루에 2~3시간 씩 뒤져봤던 것 같다. 메일을 3개 쯤 보냈고, 다행히 한 교수님께서 면담을 해보자고 답장이 오셔서 이것 저것 준비해 갔는데, 준비 하는 것 보다는 열심히 할 수 있는 자세를 계속해서 보여드리는 것이 중요한 것 같다. 연구 분야가 다르고, 연구실에 자리가 없어서 못들어갈 뻔 했다.. 다행히 연구실 구경과 연구실 가족들까지 소개 받고, 교수님께 감사의 인사를 드리고 패스트 캠퍼스 18주 과정이 끝나는 8월 중순~말 쯤 연구실에 들어가기로 했다. 물론, 그 전에 가끔 가서 눈도장을 찍어야겠지. 많이 발전한 상태로 들어가 나도 도움이 될 수 있는 사람이 되고싶다.","tags":["컨택"],"categories":["컴공대학원"]},{"title":"IT 기업에서 먹히는 자기소개서 나에게 적용!","url":"%2F2018%2F06%2F14%2FIT%20%EA%B8%B0%EC%97%85%EC%97%90%EC%84%9C%20%EB%A8%B9%ED%9E%88%EB%8A%94%20%EC%9E%90%EC%86%8C%EC%84%9C%2F","content":"\n\n\n> ### IT 기업에서 먹히는 자소서\n\n\n\n- 최근 채용 동향은 스펙보다는 Right People을 뽑는 구조이다.\n\n  - Right People이란 : **트리플 A**형 인재\n\n  - Midas IT 같은 곳은 AI 면접을 보기도 한다.\n\n\n\n- 대기업, 중견기업, 스타트업이 원하는 인재상이 모두 다르다.\n  - 대기업 : 원하는 인재상이 정확히 정해져 있다.\n  - 중견기업 : 조직 충성도, 스펙을 많이 보는 경향이 있다.\n  - **스타트업 : 기업의 비전과 뜻을 함께 하는 인재, 알아서 잘 크는 인재, 자기 주도적인 인재.**\n\n\n\n- **사실은...**\n  - 사람 됨됨이(성실성, 예의, 소통능력)\n  - 우리 회사의 '업'을 좋아하는 사람인가?\n    * 우리 회사의 업에 대해 아는가, 단순하게 취업을 하기 위한 사람인가?\n  - 기존 멤버들과의 커뮤니케이션 이슈가 없는가?\n  - 실력에 대한 기초가 탄탄한가?\n  - 가르쳐주면 빨리 배우는 사람인가?\n    * Running Curve가 가파른 사람을 좋아한다.\n  - 시키지 않아도 일을 찾아서 하는 사람인가?\n\n\n\n- 무엇부터 보는가?\n  1. 신입임에도, **경력**부터 본다.\n  2. 경력이 없다면, **경험**을 본다.\n  3. **자기소개서**는 그 다음이다. 정성들여서 쓰자.\n  4. **전공**과 **자격사항**은 1, 2, 3이 부합한다면 Pass!\n\n\n\n- 잘 쓴 자기소개서란 무엇인가?\n\n  ``` \n  당신이 원하는 것을 내가 가지고 있다!\n  ```\n\n  - 사례를 통해서 증명하여야 한다 : 계속해서 수정하여야 한다.\n    1. 왜 그 활동은 하게 되었는지?\n    2. 그 과정이 어땠는지\n    3. 그 과정을 통해 무엇을 배우고 깨달았는지\n    4. 그 결과가 어땠는지, 개선은 어땠는지\n    5. 내 삶에 어떻게 효과적으로 적용 되었는지 : 아름다운 메무뤼!\n\n  \n\n  - 스토리 예시 :\n\n    ``` \n    - 저는 학부 기간동안 토탈 25만 라인의 코드를 작성한 경험이 있습니다.\n    ```\n\n    ``` \n    - 팀 프로젝트를 진행하며 PM 롤을 수행하면서 가장 중점을 두었던 것은\n      cost-effective한 software architecture를 구성하는 것이었습니다.\n      최근 각광받고 있는 SOA 기법들을 흉내내어 활용해 보았습니다.\n    ```\n\n    ```\n    제가 자신 있는 랭귀지는 C / C++ 이고 STL를 이용한 프로젝트를 3번 이상 수행했습니다.\n    ```\n\n\n\n- **취업은, 나를 Sales 하는 것이다.**\n  - 잘 Sales 하기 위해선? \n    1. <u>제품에 대해서 잘 알고 있다</u>는 것을 어필하자.\n    2. 취업하고자 하는 <u>회사에 대해서 잘 알고 있는 것</u>을 어필하자.\n\n\n\n> ### 지피지기면 백전백승\n\n\n\n- **나를 알고** (My Case : 자기소개서를 위한 경험들을 정리해보자.)\n  - 도전적 : 문과 출신 30살 평범한 직장인에서 FastCampus Data Science 코스로의 도전.\n  - 순발력 : 노루페인트에서 영업사원으로 근무했을 때, 페인트가 아닌 다른 물건을 소싱해 3억의 매출을 올림. 자차로 전국을 돌아다니며 대형마트에 상품 진열대를 설치함.\n  - 노력 : 조별과제 PPT를 너무 못만들어서 매일 시간을 활용해 제작 실력과 나만의 Know-how를 발휘, 결국 교육부, 환경부, 롯데, CJ 등의 발표 자료를 만드는 프리랜서로 활약.\n  - 도전 : 2박 3일 자전거 국토종주를 4개월 준비 끝에 성공적으로 완료한 경험.\n  - 팀워크 : 금융의 ㄱ자도 모르는 팀원들을 이끌고 롯데카드 아이디어 공모전에서 200팀 중 2위로 입상한 경험.\n  - 선도적 : 롯데카드 인턴 당시 Co-working space와 Crowd Funding을 연계한 대기업과 중소기업 상생 서비스를 제안해 활용된 경험.\n  - 끈기 : 다나와에서 디자이너로 근무했을 때, 고객에게 제품 DB 클레임이 들어와서 3일 동안 매일 건물이 닫는 12시까지 남아 끝가지 처리 한 경험.\n  - 킹 메이커 : 내가 몸을 담고 있던 조직의 장은 내가 있는 기간 내로 승진을 하게 됨. 그만큼 업무 처리를 스무스하게 하는데 도움을 많이 주어 팀에 성과를 올림.\n\n\n\n- **적을 알자**\n  - 기업은 신입사원 한 사람을 가르치는 것에 큰 돈을 쓴다.\n  - 그렇기 때문에 기업이 원하는 바가 무엇인지 생각해서 함께 써보자.\n  - 기업은 20초만에 판단한다 : 같이 일 하고 싶은 사람, 직문 연관성이 짙은 사람.\n\n\n\n- 구직을 위한 여러 채용 사이트들!\n  - [팍스넷](http://paxnet.moneta.co.kr) : 기업에 대한 정보 Search. 특히 동종 업종 내 업체 현황이 세부적으로 나와있다.\n  - [루키잡](http://imrookie.co.kr) : 외국계 기업 및 유학생을 위한 전문 사이트. 영문 이력서 무료 첨삭 가능.\n  - [피플앤잡](http://peoplenjob.com) : 외국계 기업 찾기.\n  - [로켓펀치](http://rocketpunch.com) / [더팀스](http://theteams.kr) / [오피스엔](http://officen.kr) / [원티드](http://wanted.co.kr) : 스타트업을 위한 구직 사이트.\n\n\n\n- 효율적인 자기소개서 작성 습관?\n  - 오타는 기본. 문단 나누기, 들여쓰기 철저하게\n  - **PDF로 제출**하는 습관을 들이자 : 좋은 인상!\n  - 포트폴리오는 Github와 Google Drive로\n  - 기업명을 반드시 써주고, 그에 대해 잘 아는 늬앙스로 어필한다.\n  - 과유불급, 지나친 솔직함, 단순 나열, 서론이 긴 글은 안된다.\n    - 두괄식 작성 : 소제목, 헤드라인 활용.\n    - 자기소개서는 에세이가 아니다.\n    - 언제나 '나'가 주인공인 느낌의 주도적으로 쓰자!\n    - 외국어와 숫자를 활용, 내용 80 / 결과 20의 법칙으로 작성!\n\n\n\n- 자기 소개 문구는 강렬하게!\n\n  ```\n  아래는 예시입니다.\n  ```\n\n  \n\n  > 자투리 시간도 허투루 쓰지 않는 시간구두쇠\n\n  - 나랑 잘 맞는 내용인 것 같다.\n\n  \n\n  > 저는 포기를 잘 합니다. 하지만 포기를 좋아하지 않습니다. 포기를 자주 하진 않습니다. 말 그대로 포기를 잘 합니다.\n\n  - 나랑 잘 맞는 내용인 것 같다. 이걸 잘 다듬어보자.\n\n  \n\n  > 소제목 활용 : [타인과 소통하는 도전자], [\"Did you again?\"], [\"이 정도면 될까요?\"] 등..\n\n  - 식상하거나, 단순 나열식이거나, 허세뿐이거나 하는 제목들은 No No..\n\n    - Ex) 최고의 인재가 되기 위한 최선의 선택, 다양한 경험을 통한 실무 능력 함양 등..\n\n    \n\n  > 두괄식으로 작성하는 습관 : 저는 문제 해결능력이 뛰어납니다.\n\n\n\n- 기본 항목에 대해서 작성\n  - 해당 기업 / Position에 대한 지원 동기\n  - 가장 큰 성과 / 역경을 딛고 성취한 경험\n  - 팀 단위로 한 경험 중 본인이 수행헀던 역할과 행동\n  - 업무 강점 / 해당 업무를 위해 노력한 점\n  - 성격의 장단점\n  - 남들과 다른 관점, 창의적으로 생각한 사항\n  - 직무를 잘 하기 위해 준비한 것을 행동 위주로 쓰세요.\n    - 관련 경험이 없다면, 최근 읽은 책, 블로그를 보고 행동한 것을 작성.\n  - 자유롭게 자신을 표현해 보세요.","tags":["채용"],"categories":["취업"]},{"title":"Hexo Github 블로그 설치 초보용 최신판(180613) + 테마 추천(NexT) + 블로그 백업 방법","url":"%2F2018%2F06%2F12%2F%EC%B4%88%EB%B3%B4%EC%9A%A9%20Hexo%20%EC%84%A4%EC%B9%98%20%EC%B5%9C%EC%8B%A0%ED%8C%90(180613)%20%2B%20%ED%85%8C%EB%A7%88%20%EC%B6%94%EC%B2%9C(NexT)%20%2B%20%EB%B8%94%EB%A1%9C%EA%B7%B8%20%EB%B0%B1%EC%97%85%20%EB%B0%A9%EB%B2%95%2F","content":"\n\n\n1. 장점과 단점\n   - 장점 : \n   - 단점 : \n   - 최근 써져있는 Hexo 블로그 설치법과 추천 테마 설정, 타 PC에서 포스팅 하고 싶거나 백업 방법을 알려주고 싶어서 쓰게됨.(OS 변경이라든지, 포맷이라든지..)\n\n   \n\n2. 준비물 : Node.js, Git 설치\n   - [1. Node.js Download](https://nodejs.org/ko/download/) : OS에 맞게 8.11.2 LTS 설치\n   - [2. Git Download](https://git-scm.com/download) : OS에 맞게 Git 설치\n\n   \n\n3. **Hexo + NexT 테마 설치** :\n\n   - Git Bash를 열고 명령어를 차곡차곡 입력하겠습니다.\n\n   - 여기서부터 다른 명령어는 필요 없고 아래대로 입력만 하세요. 조금만 어긋나도 작동이 안됩니다.\n\n   - 명령이 끝나면 바로 다음 명령어를 복사해서 입력해주세요. 입력 후에는 Enter키를 눌러줍니다.\n\n     \n\n   ```\n   npm install hexo-cli -g\n   ```\n\n   - HEXO 구동틀을 PC에 설치해주는겁니다.\n\n   \n\n   ```\n   hexo init 폴더명\n   ```\n\n   - HEXO 블로그의 기반이 되는 폴더를 생성해줍니다. 여기 안에서 모든 작업들이 일어납니다.\n     - 앞으로 편의 상 폴더명을 `twosblog` 로 우선 명명하겠습니다.\n\n   \n\n   ```\n   cd 폴더명\n   ```\n\n   - 예를 들어 `cd twosblog` 입력하고 Enter키 쳐주시면 됩니다. 폴더 안으로 들어가는 겁니다.\n\n   \n\n   ```\n   npm install\n   ```\n\n   - Node.js라는 놈을 내 HEXO 블로그와 연동시켜 주는겁니다.\n\n   \n\n   ```\n   npm install hexo-generator-searchdb --save\n   ```\n\n   - `hexo-generator-searchdb` 라는 NexT 테마 안에서 검색에 쓰는 놈을 내 블로그와 연동시켜 주는겁니다.\n   - 더 좋은 검색을 위해 쓰는 플러그인이라고 생각하시면 됩니다.\n\n   \n\n   ```\n   git clone https://github.com/theme-next/hexo-theme-next themes/next\n   ```\n\n   - 폴더명/themes/next 경로에 NexT 테마가 자동으로 다운되어 설치합니다.\n\n   - 혹시 필요하실까봐 [NexT 테마 Github](https://github.com/theme-next/hexo-theme-next) 주소를 준비했습니다. 궁금하시면 접속해보세요.\n\n     \n\n   ```\n   hexo new page tags\n   hexo new page categories\n   ```\n\n   - 태그와 카테고리를 담을 폴더와 index.md 파일들을 만듭니다. 추후에 살짝 수정해줄게요.","tags":["HEXO 추천 테마"],"categories":["HEXO"]},{"title":"같은 저장소에 윈도우 10 UEFI & 우분투 18.04 BIOS 설치하면 발견되는 오류","url":"%2F2018%2F06%2F11%2F%EA%B0%99%EC%9D%80-%EC%A0%80%EC%9E%A5%EC%86%8C%EC%97%90-%EC%9C%88%EB%8F%84%EC%9A%B0-10-UEFI-%EC%9A%B0%EB%B6%84%ED%88%AC-18-04-BIOS-%EC%84%A4%EC%B9%98%ED%95%98%EB%A9%B4-%EB%B0%9C%EA%B2%AC%EB%90%98%EB%8A%94-%EC%98%A4%EB%A5%98%2F","content":"\n# GRUB, Ext2Fsd Error.\n\n이렇게 단 두가지만은 해결을 하지 못했다.\n\n### 1. GRUB Error.\n\n- 일단 제반사항은 선 윈도우 10 UEFI 설치, 후 우분투 18.04 BIOS 설치이다.\n- 정말 토할 것 같았던게, `Boot-Repair`와 Mount를 이용한 `grub-install` 방법 모두 먹히지 않는다.\n\n  ![grub-install error](https://i.imgur.com/n98agqM.png)\n- 결과적으로 이야기하자면, 윈도우 10을 UEFI로 설치하고 유분투를 BIOS로 설치한다면 *무슨 발악을 하더라도* GRUB 메뉴가 뜨지 않는다. 둘 다 BIOS로 설치를 하거나, 둘 다 UEFI로 설치해야 호환이 가능하다.\n- 이것 때문에 4시간이라는 내 피같은 시간을... 할 것도 많은데 ㅠㅠ\n\n\n\n### 2. Ext2Fsd Error.\n\n- 무엇에 대한 이야기냐면, 윈도우 10이 UEFI로 깔린 NTFS 드라이브에서 우분투가 BIOS로 깔린 EXT4 드라이브에 접근이 불가하다라는 점이다.\n- 이것 또한 결과만 말하자면 부분적으로만 가능하다. 일부 EXT4 시스템의 경우 이 프로그램을 설치하고 마운트 해도 제대로 인식이 안되는 것으로 파악되었다고 한다.(출처: <http://yourjune.tistory.com/328>)\n- `Ext2Fsd` 라는 프로그램을 깔면 쉽게 Mount가 가능하다는 블로그 글들이 있지만.. 그건 모두 같은 형태의 설치 방법을 취했을 경우이다.\n- 뻘짓에 4시간을 쓰고 나니 누구에게 빌려주고만 싶었던 나의 시간이 처음으로 아까워졌다.\n\n\n\n### 3. 결론\n\n- 윈도우와 우분투 리눅스 설치 시에는 반드시 설치 방식을 같게 설정해주세요.\n\n- 그게 싫다면 우분투는 그냥 Virtual Box를 이용해서 사용해주세요.\n\n- 그것도 싫다면 그냥 물리 디스크 한 개당 OS는 1개만 설치해서 사용해주세요.\n\n  <br>\n\n> **성공한 것** :\n>\n> - 우분투에서 윈도우 파티션 파일 인식\n>\n> **실패한 것** :\n>\n> - 윈도우 NTFS 드라이브에서 우분투 Ext4 드라이브 Mount\n>   - 물리적으로 인식시켜서 현 드라이브에 Output은 가능합니다. [DiskGenius](https://diskgenius.ko.downloadastro.com) 프로그램을 추천 드립니다.\n> - 우분투에서  GRUB으로 멀티 부팅 하기\n>   - 시스템 부팅 시 F11 등의 키로 할당되어 있는 BIOS BootLoader에 들어가셔서 Windows Boot Manager와 Ubuntu를 화살표로 조정하시면 되겠습니다. ^_ㅠ\n\n<br>\n\n- **나의 시간은 누가 보상해주나... 아마 난 안될거야...**\n- 동시에 연대정보대학원을 떨어졌다. 차라리 다행인듯? 하나에 올인해야지. 얼른 컨택컨택..","tags":["EXT4 액세스 ERROR"],"categories":["Ubuntu 18.04"]},{"title":"Algorithm Python 1 : 올바른 괄호 판단(괄호 제거)","url":"%2F2018%2F06%2F10%2FAlgorithm-Python-1-%EC%98%AC%EB%B0%94%EB%A5%B8-%EA%B4%84%ED%98%B8-%ED%8C%90%EB%8B%A8-%EA%B4%84%ED%98%B8-%EC%A0%9C%EA%B1%B0%2F","content":"\n\n\n> 출처 : 워낙 유명한 문제라 :)\n\n\n\n- 문제는 다음과 같습니다.\n\n\n\n## 올바른 괄호인지 판단하기(Python)\n\n​ 본 문제에서는 입력으로 주어지는 괄호가 올바른 괄호인지를 판단하는 프로그램을 작성합니다. 예를 들어, ‘(())’ 은 올바른 괄호이지만, ‘(()))‘, 혹은 ‘(()()(’ 는 올바른 괄호가 아닙니다.\n\n\n\n- ### 조건\n\n  - 입력 : 괄호 pp가 주어집니다.\n  - 출력 : pp가 올바른 괄호이면 YES, 그렇지 않으면 NO를 출력합니다.\n\n  \n\n- ### 예시\n\n1. 입력 예시 1 : `(())()`\n\n   출력 예시 1 : **YES**\n\n2. 입력 예시 2 : `(((())())(()())((())()))`\n\n   출력 예시 2 : **YES**\n\n3. 입력 예시 3 : `(())())()`\n\n   출력 예시 3 : **NO**\n\n\n\n- ### Python 풀이\n\n  ```pp = input()\n  pp = input()\n  pp_comp = \"\"\n  if pp.count(\"(\") != pp.count(\")\"):\n      print(\"NO\")\n  else:\n      for _ in range(len(pp)):\n          pp_comp = pp \n          pp = pp.replace(\"()\", \"\")\n          if len(pp) == len(pp_comp):\n              print(\"NO\")\n              break\n          elif len(pp) == 0:\n              print(\"YES\")\n              break\n  ```\n\n\n\n- ### 느낀 점","tags":["괄호제거"],"categories":["Algorithm"]},{"title":"TIL 1 - I can do anything.","url":"%2F2018%2F06%2F06%2FTIL-1-I-can-do-anything%2F","content":"\n# Doing today something?\n\n\n\n오늘 한 일을 내일로 미루지 말자.\n\n\n\n### 1. Ubuntu 설치\n\n- 목적은 데스크톱과 노트북의 데이터를 동기화 시키는 것. 현재 workspace가 나뉘어져 있기 때문에 각종 불편함을 가지고 있다.\n- Ubuntu Linux를 설치해 데스크톱에 있는 파일들을 노트북에서도 그대로 사용 할 수 있도록 가상화를 시키는 것이 목표.\n\n\n\n### 2. TIL : 확률 1강 및 Python Web CSS 파트 복습\n\n- Break Day 3일동안 밀린 선형대수와 미적분 파트를 복습하기 위해서는 확률 파트 복습은 실시간으로 해야한다.\n\n\n\n### 3. Hexo로 구현한 블로그 최적화와 Markdown 학습\n\n- 테마 Layout CSS 파일에 문제가 있는지, 이미지가 Resize 되지 않는 현상이 발생중이다. 이를 찾아 해결하겠노라.\n- Markdown은 Jupyter Notebook에서만 사용할 줄 알았는데, 굉장히 범용적이었다. 익혀놔서 나쁠게 1도 없을 것 같으다.\n\n\n\n------\n\n> 지금까지의 IT 관련 블로그들은 딱딱한게 특징이었다면, 문과 출신이자 디자인(?)을 다뤄본 필자의 입장에서는 다소 말랑말랑한 어투로 각종 툴들의 Preview를 다뤄볼까 한다. 물론 쉽지 않겠지만ㅠ\n\n------\n\n\n\n지하철 이동 시간엔 항상 Github 블로그와 Markdown을 다루는데 주력하겠노라!","tags":["Github"],"categories":["TIL"]}]